simbetas <- mvrnorm(sims, arima.cesd.pe, arima.cesd.vc) #needs library(MASS)
cesd_scen <- seq(1,3,1)
n_scen <- length(cesd_scen)
zero_scen<-wave2_scen<-wave3_scen<-cfMake(model,data,n_scen) # from simcf
zero_scen<-cfMake(model,data,n_scen) # from simcf
model<-arima(cesd, order = c(1,0,0),
xreg = xcovariates, include.mean = TRUE
)
data<-csss594
sims <- 10000
simbetas <- mvrnorm(sims, arima.cesd.pe, arima.cesd.vc) #needs library(MASS)
cesd_scen <- seq(1,3,1)
zero_scen<-wave2_scen<-wave3_scen<-cfMake(model,data,n_scen) # from simcf
source('~/Dropbox/Current Projects/PSAR/csss594/csss594_5-22-15.R', echo=TRUE)
# updated: 5/24/2015
rm(list=ls())
csss594 <- read.csv('~/Dropbox/Current Projects/PSAR/csss594/csss594.csv', header =T)
#csss594 <- read.csv("C:/Users/Juliemmor/Dropbox/PSAR/csss594/csss594.csv", header=T)
attach(csss594)
# Load libraries
library(nlme)      # Estimation of mixed effects models
library(lme4)      # Alternative package for mixed effects models
library(plm)       # Econometrics package for linear panel models
library(arm)       # Gelman & Hill code for mixed effects simulation
library(pcse)      # Calculate PCSEs for LS models (Beck & Katz)
library(tseries)   # For ADF unit root test
library(simcf)     # For panel functions and simulators
library(forecast)     # For auto.arima and cross-validation
library(lmtest)       # For Breusch-Godfrey LM test of serial correlation
library(RColorBrewer) # For nice colors
library(urca)              # For estimating cointegration models
library(MASS)              # For mvrnorm()
library(Zelig)             # For approval data
library(quantmod)          # For creating lags
source("TSplotHelper.R")   # Helper function for counterfactual time series plots
library(tile)
ts.cesd <- ts(cesd, start=c(1), end=c(3), frequency=1)
ts.cesd
plot(ts.cesd)
#check for unit root
PP.test(cesd)
#indicates stationary
adf.test(cesd)
#Still suggests stationary
#####################################
# Because our data are stationary, DO NOT need to take the difference
#For shits, and giggles, here it is...
#Use Lag function to lag
cesdlag<- lagpanel(cesd, idx, year,1)
csss594$cesdlag <- cesdlag
#First differences: level - lagged level
cesdDiff <- cesd - cesdlag
cesdLagDiff <- cbind(cesd, cesdlag, cesdDiff)
cesdLagDiff
csss594$cesdDiff <- cesdDiff
csss594$cesdLagDiff <- cesdLagDiff
#Check for unit root after difference
PP.test(as.vector(na.omit(cesdDiff)))
#indicates stationary after differencing
adf.test(na.omit(cesdDiff))
#Suggests our data are stationary
#####################################
#since data are stationary, we will look at ARIMA models
acf(cesd)
#looks auto-regressive; perhaps AR(2)
pacf(cesd)
#suggests it's more likely an AR(1); does not suggest an MA process
## Model cesd1b:  ARIMA(1,0,0) model of cesd with change in household structure as a covariate
xcovariates <- cbind(two_parents_change_w)
arima.cesd <- arima(cesd, order = c(1,0,0),
xreg = xcovariates, include.mean = TRUE
)
print(arima.cesd)
# Extract estimation results from arima.res1a
arima.cesd.pe <- arima.cesd$coef                    # parameter estimates (betas)
arima.cesd.vc <- vcov(arima.cesd)                    # covariance
arima.cesd.se <- sqrt(diag(arima.cesd$var.coef))    # standard errors
arima.cesd.ll <- arima.cesd$loglik                  # log likelihood at its maximum
arima.cesd.sigma2hat <- arima.cesd$sigma2           # standard error of the regression
arima.cesd.aic <- arima.cesd$aic                    # Akaike Information Criterion
arima.cesd.resid <- arima.cesd$resid                # residuals
###We are not doing out-of-model prediction because household structure changes in adulthood, we argue, are less impactful to mental health than in adolescence.
model<-arima(cesd, order = c(1,0,0),
xreg = xcovariates, include.mean = TRUE
)
data<-csss594
sims <- 10000
simbetas <- mvrnorm(sims, arima.cesd.pe, arima.cesd.vc) #needs library(MASS)
#This is the range of your variable aross which you want to see increases; something continuous
cesd_scen <- seq(1,3,1)
n_scen <- length(cesd_scen)
model<-lm(cesd~two_parents_change_w)
zero_scen<-cfMake(model,data,n_scen) # from simcf
data<-csss594
model<-lm(cesd~two_parents_change_w)
sims <- 10000
simbetas <- mvrnorm(sims, arima.cesd.pe, arima.cesd.vc) #needs library(MASS)
cesd_scen <- seq(1,3,1)
n_scen <- length(cesd_scen)
zero_scen<-wave2_scen<-wave3_scen<-cfMake(model,data,n_scen) # from simcf
zero_scen<-cfMake(model,data,n_scen) # from simcf
for (i in 1:n_scen) {
zero_scen <- cfChange(zero_scen, "two_parents_change_w",x=0,scen=i)
zero_scen <- cfChange(zero_scen, "Year",x=i,scen=i)
# change in wave2
wave2_scen <- cfChange(wave2_scen, "two_parents_change_w",x=1,scen=2)
wave2_scen <- cfChange(wave2_scen, "Year",x=i,scen=i)
# change in wave3
wave3_scen <- cfChange(wave3_scen, "two_parents_change_w",x=1,scen=3)
wave3_scen <- cfChange(wave3_scen, "Year",x=i,scen=i)
}
#print(wave2_scen)
#Produces predicted probabilities using the above CF scenarios, one for each racial category from BMI=14-41
zero_scen.sim <- simev(zero_scen, simbetas, ci=0.95)
wave2_scen.sim <- simev(wave2_scen, simbetas, ci=0.95)
wave3_scen.sim <- simev(wave3_scen, simbetas, ci=0.95)
phi <- 0.36 # from model summary()
lagY <- mean(cesdDiff) # Hypothetical previous change in Y for simulation
initialY <- mean(cesd) # Hypothetical initial level of Y for simulation
zero_scen_sim <- ldvsimev(zero_scen,               # The matrix of hypothetical x's
simbetas,           # The matrix of simulated betas
ci=0.95,            # Desired confidence interval
constant=NA,        # NA indicates no constant!
phi=phi,            # estimated AR parameters; length must match lagY
lagY=lagY,          # lags of y, most recent last
initialY=initialY   # for differenced models, the lag of the level of y
)
length(zero_scen)
length(phi)
length(cesd)
length(cesdDiff)
length(lagY)
length(initialY)
print(initalY)
print(initialY)
zero_scen_sim <- ldvsimev(zero_scen,               # The matrix of hypothetical x's
simbetas,           # The matrix of simulated betas
ci=0.95,            # Desired confidence interval
phi=phi,            # estimated AR parameters; length must match lagY
lagY=lagY,          # lags of y, most recent last
initialY=initialY   # for differenced models, the lag of the level of y
)
zero_scen_sim <- ldvsimev(zero_scen,               # The matrix of hypothetical x's
simbetas,           # The matrix of simulated betas
ci=0.95,            # Desired confidence interval
)
zero_scen_sim <- ldvsimev(zero_scen,               # The matrix of hypothetical x's
simbetas,           # The matrix of simulated betas
ci=0.95,            # Desired confidence interval
constant=1,
phi=phi,            # estimated AR parameters; length must match lagY
lagY=lagY,          # lags of y, most recent last
initialY=initialY   # for differenced models, the lag of the level of y
)
zero_scen_sim <- ldvsimev(zero_scen,               # The matrix of hypothetical x's
simbetas,           # The matrix of simulated betas
ci=0.95,            # Desired confidence interval
)
zero_scen
length(simbetas)
model<-lm(cesd~two_parents_change_w)
model.pe<-model$coef
model.vc<-vcov(model)
sims <- 10000
simbetas <- mvrnorm(sims, model.pe, model.vc) #needs library(MASS)
cesd_scen <- seq(1,3,1)
n_scen <- length(cesd_scen)
zero_scen<-wave2_scen<-wave3_scen<-cfMake(model,data,n_scen) # from simcf
# change in zero waves
for (i in 1:n_scen) {
zero_scen <- cfChange(zero_scen, "two_parents_change_w",x=0,scen=i)
zero_scen <- cfChange(zero_scen, "Year",x=i,scen=i)
# change in wave2
wave2_scen <- cfChange(wave2_scen, "two_parents_change_w",x=1,scen=2)
wave2_scen <- cfChange(wave2_scen, "Year",x=i,scen=i)
# change in wave3
wave3_scen <- cfChange(wave3_scen, "two_parents_change_w",x=1,scen=3)
wave3_scen <- cfChange(wave3_scen, "Year",x=i,scen=i)
}
phi <- 0.36 # from model summary()
lagY <- mean(cesdDiff) # Hypothetical previous change in Y for simulation
initialY <- mean(cesd) # Hypothetical initial level of Y for simulation
zero_scen_sim <- ldvsimev(zero_scen,               # The matrix of hypothetical x's
simbetas,           # The matrix of simulated betas
ci=0.95,            # Desired confidence interval
constant=1,
phi=phi,            # estimated AR parameters; length must match lagY
lagY=lagY,          # lags of y, most recent last
initialY=initialY   # for differenced models, the lag of the level of y
)
zero_scen_sim <- ldvsimev(zero_scen,               # The matrix of hypothetical x's
simbetas,           # The matrix of simulated betas
ci=0.95,            # Desired confidence interval
phi=phi,            # estimated AR parameters; length must match lagY
lagY=lagY,          # lags of y, most recent last
initialY=initialY   # for differenced models, the lag of the level of y
)
model.pe<-model$coef
model.vc<-vcov(model)
zero_scen_sim <- ldvsimev(zero_scen,               # The matrix of hypothetical x's
simbetas,           # The matrix of simulated betas
ci=0.95,            # Desired confidence interval
phi=phi,            # estimated AR parameters; length must match lagY
lagY=lagY,          # lags of y, most recent last
initialY=initialY   # for differenced models, the lag of the level of y
)
zero_scen_sim <- ldvsimev(zero_scen,               # The matrix of hypothetical x's
simbetas,           # The matrix of simulated betas
ci=0.95,            # Desired confidence interval
phi=phi,            # estimated AR parameters; length must match lagY
lagY=lagY,          # lags of y, most recent last
initialY=initialY   # for differenced models, the lag of the level of y
)
install.packages(c("curl", "lmtest", "RcppArmadillo"))
install.packages("curl")
install.packages("curl")
install.packages("curl")
install.packages("curl")
install.packages(c("ergm", "manipulate", "plyr", "rmarkdown", "robustbase", "scales", "shiny", "tergm"))
install.packages("manipulate")
install.packages(c("chron", "curl", "ergm.count", "latentnet", "manipulate"))
install.packages(c("chron", "manipulate"))
install.packages(c("igraph", "manipulate"))
install.packages(c("igraph", "manipulate"))
install.packages('RJSONIO)
)
:
9
""
)
:
()
stop()
)
}
{}
}
install.packages('RJSONIO')
#### This script uses RCurl and RJSONIO to download data from Google's API:
#### Latitude, longitude, location type (see explanation at the end), formatted address
#### Notice ther is a limit of 2,500 calls per day
library(RCurl)
library(RJSONIO)
library(plyr)
url <- function(lat, lon, return.call = "json") {
root <- "https://maps.googleapis.com/maps/api/distancematrix/"
n = nrow(lat)
origins = paste(lat[1],lon[1], sep = ",")
dests = paste(lat[2], lon[2], sep = ",")
key = "AIzaSyAIGBjTozWEfIvalGkg7XDWkRvRf97_JFE"
u <- paste(root, return.call, "?origins=", origins, "&destinations=", dests,"&key=", key, sep = "")
return(URLencode(u))
}
distanceMatrix <- function(lat, lon,verbose=FALSE) {
if(verbose) cat(address,"\n")
u <- url(lat, lon)
doc <- getURL(u)
x <- fromJSON(doc,simplify = FALSE)
if(x$status=="OK") {
dur <- x$rows[[1]]$elements[[1]]$duration$value
dist <- x$rows[[1]]$elements[[1]]$distance$value
return(c(dur, dist))
} else {
stop("ERROR: Google Distance Matrix API did not return data.")
}
}
install.packages(c('openxlsx', 'timeDate'))
library(openxlsx)
library(dplyr)
library(timeDate)
library(RJSONIO)
options(digits = 8)
#delete pre-existing .csv data file
files <- list.files()
if("UW_Trip_Data_QC.csv" %in% files){file.remove("UW_Trip_Data_QC.csv")}
if (!("AD" %in% ls())){AD = read.csv("UW_Trip_Data.csv")}
AD_56 = AD[which(AD$ProviderId==5 | AD$ProviderId==6),]
AD_56$Run <-as.character(AD_56$Run)
AD_56 <- AD_56[which(!is.na(AD_56$Run)),]
AD_56$ServiceDate <- as.timeDate(as.character(AD_56$ServiceDate))
ctr = 1; activ = AD_56$Activity[ctr]
while(activ != 4){
ctr = ctr+1
activ = AD_56$Activity[ctr]
}
AD_56 = AD_56[-(1:(ctr-1)),]
#Compute ECDF of distance per leg
dates = unique(AD_56$ServiceDate)
rides = unique(AD_56$Run)
ETA_hist_vec = numeric(1)
Dists_hist_vec = ETA_hist_vec
#Haversine formula: "As the crow flies" distance
deg2rad <- function(deg){return(deg*pi/180)}
gcd.hf <- function(lat, lon){
long1 <- deg2rad(lon[1]); long2 <- deg2rad(lon[2])
lat1 <- deg2rad(lat[1]); lat2 <- deg2rad(lat[2])
R <- 6371 # Earth mean radius [km]
delta.long <- (long2 - long1)
delta.lat <- (lat2 - lat1)
a <- sin(delta.lat/2)^2 + cos(lat1) * cos(lat2) * sin(delta.long/2)^2
c <- 2 * asin(min(1,sqrt(a)))
d = R * c
return(d) # Distance in km
}
#Coordinate boundaries of King County, WA:
upper_right <- c(49.020430, -116.998768)
lower_left <- c(45.606961, -124.974842)
minlat = lower_left[1]; maxlat = upper_right[1]
minlon = lower_left[2]; maxlon = upper_right[2]
#Flag routes accordingly
for (ride in rides){ #iterate over every instance of a route
temp_ride = AD_56[which(AD_56$Run == ride),]
temp_ride_days = unique(temp_ride$ServiceDate)
for(k in 1:length(temp_ride_days)){ #iterate over one route, different days
flag = "OK"
this_ride = temp_ride[which(temp_ride$ServiceDate==temp_ride_days[k]),]
lat_coords = this_ride$LAT; lon_coords = this_ride$LON
if(any(lat_coords<minlat) | any(lat_coords>maxlat) | any(lon_coords<minlon) | any(lon_coords>maxlon)){
flag = "BAD_COORDS"
}
for(kk in 1:nrow(this_ride)){
if((!is.na(this_ride$BookingId[kk])) & (is.na(this_ride$ClientId[kk]))){
if(flag != "OK"){
flag = paste(flag, "MISSING_CLIENTID", sep = ", ")
}
else{
flag = "MISSING_CLIENTID"
}
}
}
if(all(this_ride$LON==this_ride$LON[1])){
flag = "NO_MOVEMENT"
}
Durs = numeric(nrow(this_ride)-1)
Dists = numeric(nrow(this_ride)-1)
for (leg in 2:nrow(this_ride)){
lat = c(this_ride$LAT[leg-1], this_ride$LAT[leg]); lon = c(this_ride$LON[leg-1], this_ride$LON[leg])
Durs[leg-1] = this_ride$ETA[leg]-this_ride$ETA[leg-1]
Dists[leg-1] = gcd.hf(lat, lon)
}
if(this_ride$Activity[1]!=4 | this_ride$Activity[nrow(this_ride)]!=3){
if(flag != "OK"){
flag = paste(flag, "ROUTE_FINISH_ERROR", sep = ", ")
}
else{
flag = "ROUTE_FINISH_ERROR"
}
}
if(flag == "OK"){
ETA_hist_vec <- c(ETA_hist_vec, Durs)
Dists_hist_vec <- c(Dists_hist_vec, Dists)
write.table(this_ride, file = "UW_Trip_Data_QC.csv", col.names = F, append = T, sep = ",")
}
#     LATs = this_ride$LAT; LONs = this_ride$LON
#     SCHEDs = this_ride$SchedStatus
#     CLIENTIDs = this_ride$ClientId
#     ride_list = list(ROUTE = ride, DATE = as.character(temp_ride_days[k]), FLAG = flag, DURATIONS = Durs, EUCLID_DISTS = Dists,
#                      LAT = LATs, LON = LONs, SCHED_STAT = SCHEDs, CLIENTID = CLIENTIDs, PASSON = as.character(this_ride$PassOn),
#                      PASSOFF = as.character(this_ride$PassOff), ETA = this_ride$ETA, ACTIVITY = this_ride$Activity)
#     write(toJSON(ride_list), "UW_Trip_QC.json", append = T)
}
}
# #Plot ATCF distances/time of legs
# plot(log(ETA_hist_vec), Dists_hist_vec,
#     xlab = "Time", ylab = "Euclidean Distance", main= "Time vs. Distance Scatterplot",
#     pch = 21, bg = "red")
#
# #Note outliers: time splits range from 0 to 304201. 99.5% quantile is 7824.4 seconds.
# plot(ecdf(ETA_hist_vec[which(ETA_hist_vec < quantile(ETA_hist_vec, .999))]/60),
#      xlab = "Minutes", ylab = "ECDF of Stop Splits", main = "ECDF of Travel Times between Stops")
install.packages('dplyr')
# RIDGE REGRESSION
# Model is E(Y) = 0 + 1 X1 + 1 X2 + e   with e~N(0,1)
# Three variables are measured: x1,x2,x3.
#  x1 and x1 are U(0,1); x3=10 * X1 + unif(0,1).
#   This causes corr(X1,X3)=sqrt(100/101)=0.995.
# We will fit OLS and ridge regressions to these data,
#  use the data to select the "best" constant to add,
#  and then evaluate the two regressions on a new test set.
# Ridge regression function, ridge.lm(), is on MASS package
library(MASS)
# Generating the data
set.seed(558562316)
N <- 20      # Sample size
x1 <- runif(n=N)
x2 <- runif(n=N)
x3 <- runif(n=N)
x3c <- 10*x1 + x3 # New variable
ep <- rnorm(n=N)
y <- x1 + x2 + ep
# OLS fit of 3-variable model using independent x3
ols <- lm(y~ x1 + x2 + x3)
summary(ols)
# OLS fit of 3-variable model using correlated x3.
olsc <- lm(y~ x1 + x2 + x3c)
summary(olsc)
# Ridge regression using independent variables
ridge <- lm.ridge (y ~ x1+x2+x3, lambda = seq(0, .1, .001))
summary(ridge)
plot(ridge)
# Ridge regression using correlated variables
ridgec <- lm.ridge (y ~ x1+x2+x3c, lambda = seq(0, .1, .001))
plot(ridgec)
select(ridgec)
# Selection of constant is at endpoint.  Extend endpoint and try again
ridgec <- lm.ridge (y ~ x1+x2+x3c, lambda = seq(0, 1, .1))
plot(ridgec)
select(ridgec)
# Selection of constant is at endpoint.  Extend endpoint and try again
ridgec <- lm.ridge (y ~ x1+x2+x3c, lambda = seq(0, 10, .01))
plot(ridgec)
select(ridgec)
# Final model uses lambda=4.
ridge.final <- lm.ridge (y ~ x1+x2+x3c, lambda = 4)
ridge.final
summary(ridge.final)
# Create test data and compute predicted values for OLS and ridge.
#  There's no predict() method for "ridgelm" objects
test <- expand.grid(x1 = seq(.05,.95,.1), x2 = seq(.05,.95,.1), x3=seq(.05,.95,.1))
mu <- test$x1 + test$x2
test$x3c <- 10*test$x1 + test$x3
pred.ols <- predict(ols,newdata=test)   # y ~ X1 + X2 + X3
pred.olsc <- predict(olsc,newdata=test) # y ~ X1 + X2 + X3c
pred.ridge <- coef(ridge.final)[1] + coef(ridge.final)[2]*test[,1] +
coef(ridge.final)[3]*test[,2] + coef(ridge.final)[4]*test[,4]
MSPE.ols <- sum((pred.ols - mu)^2)/length(mu)
MSPE.olsc <- sum((pred.olsc - mu)^2)/length(mu)
MSPE.ridge <- sum((pred.ridge - mu)^2)/length(mu)
MSPE.ols
MSPE.olsc
MSPE.ridge
plot(ridge)
plot(ridgec)
plot(ridgec)
select(ridgec)
MSPE.ols
MSPE.olsc
MSPE.ridge
norm(ridge$coef,type = "2")
norm(ols$coefficients, type = "2")
norm(ridge.final$coef,type = "2")
ridge.final$coef
ols$coefficients
install.packages(c("manipulate", "RCurl", "XML"))
install.packages("manipulate")
### Make linear regression, regressors vs. total route run time ####
library(timeDate)
####################################################################################################
############################### Model Building #####################################################
####################################################################################################
setwd('~/Dropbox/Current Projects/DSSG/access/main_repo/Access_Analysis_Rproject/data')
meta_data <- read.csv("ride_meta_data.txt", sep = ",")
meta_data <- meta_data[which(meta_data$elapsed_time<86400),]
data <- read.csv("UW_Trip_Data_QC.csv")
### Make linear regression, regressors vs. total route run time ####
library(timeDate)
####################################################################################################
############################### Model Building #####################################################
####################################################################################################
setwd('~/Dropbox/Current Projects/DSSG/access/main_repo/Access_Analysis_Rproject/data')
meta_data <- read.csv("ride_meta_data.txt", sep = ",")
meta_data <- meta_data[which(meta_data$elapsed_time<86400),]
data <- read.csv("UW_Trip_Data_QC.csv")
# Make matrix of binary contrasts. Every column represents a city, 0 or 1
# indicates whether a route on a certain day hits a city.
cities <- unique(as.character(data$City))
cities <- cities[cities!=""]
city_mat <- matrix(0, nrow = nrow(meta_data), ncol = length(cities))
colnames(city_mat) <- cities
rownames(city_mat) <- meta_data$run
data$ServiceDate <- as.timeDate(as.character(data$ServiceDate))
data$Run <- as.character(data$Run)
dates <- unique(data$ServiceDate)
runs <- unique(data$Run)
ctr <- 1
for(ii in 1:length(runs)){
temp <- data[which(data$Run == runs[ii]),]
temp_days <- unique(temp$ServiceDate)
for (kk in 1:length(temp_days)){
this_run <- temp[which(temp$ServiceDate==temp_days[kk]),]
visited = unique(this_run$City[this_run$City != ""])
city_mat[ctr,match(visited, cities)] <- 1
ctr <- ctr +1
}
}
city_df <- as.data.frame(city_mat)
colnames(city_df) <- cities
# Total run time is a function of maximum clients serviced, average dwell time,
# and cities hit.
mod1 <- lm(formula = meta_data$elapsed_time ~ meta_data$max_num_pass + meta_data$avg_dwell_time + city_mat)
sumry <- summary(mod1)
print(sumry)
hist(meta_data$elapsed_time - mod1$fitted.values)
city_coefs <- {}
for (i in seq(3:51)){
city_coefs[i]<-mod1$coefficients[i]
if (summary(mod1)$coefficients[,4][i]>.05)
city_coefs[i]<-NA
}
city_rank<-order(city_coefs, decreasing=FALSE)
names(city_rank)<-cities
# show high/low cost cities
# high cost
high_cost<-which(city_rank<10)
high_cost_cities<-names(city_rank)[city_rank<10]
for (i in high_cost_cities){
print(mean(meta_data$avg_dwell_time))
}
# low cost
low_cost<-which(city_rank>40)
low_cost_cities<-names(city_rank)[city_rank>40]
View(data)
